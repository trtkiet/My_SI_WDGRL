from mpmath import mp
import numpy as np
import torch
from typing import List

mp.dps = 500
def gen_data(mu: float, delta: List[int], n: int, d: int):
    mu = np.full((n, d), mu, dtype=np.float64)
    noise = np.random.normal(loc = 0, scale = 1, size=(n, d))
    X = mu + noise
    labels = np.zeros(n)
    if len(delta) == 1:
        n_anomalies = int(n * 0.05)
        idx = np.random.choice(n, n_anomalies, replace=False)
        X[idx] = X[idx] + delta[0]
        if delta[0] != 0:
            labels[idx] = np.ones(n_anomalies)
    else:
        # In this case, we generate data for source domain.
        # 5% of the data is abnormal.
        # Anomalies are generated by randomly adding deltas to the data.
        n_anomalies = int(n * 0.05)
        idx = np.random.choice(n, n_anomalies, replace=False)
        if 0 in delta: 
            delta.pop(delta.index(0))
        if len(delta) != 0:
            split_points = sorted(np.random.choice(range(1, len(idx)), len(delta) - 1, replace=False))
            segments = np.split(idx, split_points)
            for i, segment in enumerate(segments):
                X[segment] = X[segment] + delta[i]
            labels[idx] = 1
    return X, labels

def intersect(itv1, itv2):
    # print(itv1, itv2)
    itv = [max(itv1[0], itv2[0]), min(itv1[1], itv2[1])]
    if itv[0] > itv[1]:
        return None    
    return itv

def solve_linear_inequality(u, v): #u + vz < 0
    u = float(u)
    v = float(v)
    # print(u, v)
    if (v > -1e-16 and v < 1e-16):
        if (u <= 0):
            return [-np.Inf, np.Inf]
        else:
            print('error', u, v)
            return None
    if (v < 0):
        return [-u/v, np.Inf]
    return [np.NINF, -u/v]

def get_dnn_interval(Xtj, a, b, model):
    layers = []

    for name, param in model.named_children():
        temp = dict(param._modules)
        
        for layer_name in temp.values():
            if ('Linear' in str(layer_name)):
                layers.append('Linear')
            elif ('ReLU' in str(layer_name)):
                layers.append('ReLU')

    ptr = 0
    itv = [np.NINF, np.Inf]
    u = a
    v = b
    temp = Xtj
    weight = None
    bias = None
    for name, param in model.named_parameters():
        if (layers[ptr] == 'Linear'):
            if ('weight' in name):
                weight = param.data.cpu().detach().numpy()
            elif ('bias' in name):
                bias = param.data.cpu().detach().numpy().reshape(-1, 1)
                ptr += 1
                temp = weight.dot(temp) + bias
                u = weight.dot(u) + bias
                v = weight.dot(v)

        if (ptr < len(layers) and layers[ptr] == 'ReLU'):
            ptr += 1
            Relu_matrix = np.zeros((temp.shape[0], temp.shape[0]))
            sub_itv = [np.NINF, np.inf]
            for i in range(temp.shape[0]):
                if temp[i] > 0:
                    Relu_matrix[i][i] = 1
                    sub_itv = intersect(sub_itv, solve_linear_inequality(-u[i], -v[i]))
                else:
                    sub_itv = intersect(sub_itv, solve_linear_inequality(u[i], v[i]))
            itv = intersect(itv, sub_itv)
            temp = Relu_matrix.dot(temp)
            u = Relu_matrix.dot(u)
            v = Relu_matrix.dot(v)
    return itv, u[:, 0], v[:, 0]

def get_alpha_percent_greatest(X, alpha):
    return np.argsort(X)[-int(alpha*len(X))]

def AE_AD(Xs, Xt, ae, alpha):
    X = torch.cat((Xs.cuda(), Xt.cuda()), 0)
    reconstruction_loss = ae.reconstruction_loss(X)
    reconstruction_loss = [i.item() for i in reconstruction_loss]
    #Take 5% largest in reconstruction loss
    O = np.argsort(reconstruction_loss)[-int(alpha*len(reconstruction_loss)):]
    O = [i - Xs.shape[0] for i in O if i >= Xs.shape[0]]
    return np.sort(O)

def get_ad_interval(X, X_hat, X_tilde, reconstruction_loss, a, b, wdgrl, ae, alpha):
    itv = [np.NINF, np.Inf]
    itv = [np.NINF, np.Inf]
    u = np.zeros((X.shape[0], X_hat.shape[1]))
    v = np.zeros((X.shape[0], X_hat.shape[1]))
    p = np.zeros((X.shape[0], X_hat.shape[1]))
    q = np.zeros((X.shape[0], X_hat.shape[1]))
    s = np.zeros((X.shape[0], X_hat.shape[1]))
    O = []
    for i in range(X_hat.shape[0]):
        sub_itv, u[i], v[i] = get_dnn_interval(X[i].reshape(-1, 1), a[i].reshape(-1, 1), b[i].reshape(-1, 1), wdgrl.generator)
        itv = intersect(itv, sub_itv)
    for i in range(X_hat.shape[0]):
        sub_itv, p[i], q[i] = get_dnn_interval(X_hat[i].reshape(-1, 1), u[i].reshape(-1, 1), v[i].reshape(-1, 1), ae)
        itv = intersect(itv, sub_itv)
    for i in range(X_hat.shape[0]):
        for d in range(X_hat.shape[1]):
            if X_tilde[i, d] < X_hat[i, d]:
                itv = intersect(itv, solve_linear_inequality(p[i, d] - u[i, d], q[i, d] - v[i, d]))
            else:
                itv = intersect(itv, solve_linear_inequality(u[i, d] - p[i, d], v[i, d] - q[i, d]))
            s[i, d] = np.sign(X_tilde[i, d].cpu() - X_hat[i, d].cpu())
    
    
    pivot = get_alpha_percent_greatest(reconstruction_loss, alpha)
    A = np.zeros((X_hat.shape[0], 1))
    B = np.zeros((X_hat.shape[0], 1))
    for i in range(u.shape[0]):
        for d in range(u.shape[1]):
            A[i] += s[i, d]*(p[i, d] - u[i, d])
            B[i] += s[i, d]*(q[i, d] - v[i, d])
    for i in range(X_hat.shape[0]):
        if reconstruction_loss[i] < reconstruction_loss[pivot]:
            itv = intersect(itv, solve_linear_inequality(A[i] - A[pivot], B[i] - B[pivot]))
        else:
            itv = intersect(itv, solve_linear_inequality(A[pivot] - A[i], B[pivot] - B[i]))
    return itv

def compute_yz(X, etaj, zk, n):
    sq_norm = (np.linalg.norm(etaj))**2

    e1 = np.identity(n) - (np.dot(etaj, etaj.T))/sq_norm
    a = np.dot(e1, X)

    b = etaj/sq_norm

    Xz = a + b*zk

    return Xz, a, b

def max_sum(X):
    return 0

def parametric_si(Xz, a, b, zk, wdgrl, ae, alpha, ns):
    Xz = torch.FloatTensor(Xz).cpu()
    Xz_hat = wdgrl.extract_feature(Xz.to(wdgrl.device)).cpu()
    Xz_tilde = ae.forward(Xz_hat.to(ae.device)).cpu()
    reconstruction_loss = ae.reconstruction_loss(Xz_hat.to(ae.device))
    reconstruction_loss = [i.item() for i in reconstruction_loss]
    Oz = AE_AD(Xz_hat[:ns], Xz_hat[ns:], ae, alpha)
    itv = get_ad_interval(Xz, Xz_hat, Xz_tilde, reconstruction_loss, a, b, wdgrl, ae, alpha)
    return itv[1] - min(zk, itv[1]), Oz


def run_parametric_si(X, etaj, n, threshold, wdgrl, ae, alpha, ns):
    zk = -threshold

    list_zk = [zk]
    list_Oz = []

    while zk < threshold:
        Xz, a, b = compute_yz(X, etaj, zk, n)
        skz, Oz = parametric_si(Xz, a, b, zk, wdgrl, ae, alpha, ns)
        zk = zk + skz + 1e-3 
        # zk = min(zk, threshold)
        list_zk.append(zk)
        list_Oz.append(Oz)
        # print(f'intervals: {zk-skz-1e-3} - {zk -1e-3}')
        # print(f'Anomaly index: {Oz}')
        # print('-------------')
    return list_zk, list_Oz
        
def cdf(mu, sigma, list_zk, list_Oz, etajTX, O):
    numerator = 0
    denominator = 0
    for each_interval in range(len(list_zk) - 1):
        al = list_zk[each_interval]
        ar = list_zk[each_interval + 1] - 1e-3

        if (np.array_equal(O, list_Oz[each_interval]) == False):
            continue

        denominator = denominator + mp.ncdf((ar - mu)/sigma) - mp.ncdf((al - mu)/sigma)
        if etajTX >= ar:
            numerator = numerator + mp.ncdf((ar - mu)/sigma) - mp.ncdf((al - mu)/sigma)
        elif (etajTX >= al) and (etajTX< ar):
            numerator = numerator + mp.ncdf((etajTX - mu)/sigma) - mp.ncdf((al - mu)/sigma)
    # print(f'numerator: {numerator}')
    # print(f'denominator: {denominator}')
    if denominator != 0:
        return float(numerator/denominator)
    else:
        return None

def get_interval(Xtj, a, b, model):
    layers = []

    for name, param in model.generator.named_children():
        temp = dict(param._modules)
        
        for layer_name in temp.values():
            if ('Linear' in str(layer_name)):
                layers.append('Linear')
            elif ('ReLU' in str(layer_name)):
                layers.append('ReLU')

    ptr = 0
    itv = [np.NINF, np.Inf]
    u = a
    v = b
    temp = Xtj
    weight = None
    bias = None
    for name, param in model.generator.named_parameters():
        if (layers[ptr] == 'Linear'):
            if ('weight' in name):
                weight = param.data.cpu().detach().numpy()
            elif ('bias' in name):
                bias = param.data.cpu().detach().numpy().reshape(-1, 1)
                ptr += 1
                temp = weight.dot(temp) + bias
                u = weight.dot(u) + bias
                v = weight.dot(v)

        if (ptr < len(layers) and layers[ptr] == 'ReLU'):
            ptr += 1
            Relu_matrix = np.zeros((temp.shape[0], temp.shape[0]))
            sub_itv = [np.NINF, np.inf]
            for i in range(temp.shape[0]):
                if temp[i] > 0:
                    Relu_matrix[i][i] = 1
                    sub_itv = intersect(sub_itv, solve_linear_inequality(-u[i][0], -v[i][0]))
                else:
                    sub_itv = intersect(sub_itv, solve_linear_inequality(u[i][0], v[i][0]))
            itv = intersect(itv, sub_itv)
            temp = Relu_matrix.dot(temp)
            u = Relu_matrix.dot(u)
            v = Relu_matrix.dot(v)

    return itv, u, v

def truncated_cdf(etajTy, mu, sigma, left, right):
    numerator = mp.ncdf((etajTy - mu) / sigma) - mp.ncdf((left - mu) / sigma)
    denominator = mp.ncdf((right - mu) / sigma) - mp.ncdf((left - mu) / sigma)
    # print(f'numerator: {numerator}')
    # print(f'denominator: {denominator}')
    if denominator != 0:
        return float(numerator/denominator)
    else:
        return None